{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Members:\n",
    "\n",
    "- Name, matriculation number\n",
    "- Name, matriculation number\n",
    "- Name, matriculation number"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10: Predict Stock Price\n",
    "\n",
    "In this assignment we will construct a simple recurrent network with a single hidden layer: a long short-term memory network (LSTM).\n",
    "This network will be trained on historical stock market datasets of two distinct companies, and our objective is to leverage them for predicting their future stock prices.\n",
    "\n",
    "Given that our dataset is non-textual, we will skip any text-based data transformations during the network training process, focusing only on normalization of the data.\n",
    "Basically, the dataset comprises the daily closing prices of a stock in the market spanning from early 2000 to 2021.\n",
    "Since we will be working with a single time series feature, it is important to select the appropriate dimension $D$ to represent the stock price at each time step.\n",
    "\n",
    "Price sequences of length $S$ will, hence, be turned into matrices of size $\\mathbf X = \\{\\vec x^{\\{s\\}}, 1 \\leq s\\leq S\\} \\in \\mathbb R^{S\\times D}$.\n",
    "For each input, you should provide the target value $\\mathbf T$ based on a sequence length $S$, where the target for each sample corresponds to the price of the next time step: $\\vec t^{\\{s\\}} = \\vec x ^{\\{s+1\\}}$.\n",
    "\n",
    "To speed up processing, these sequences will be put into batches, i.e., $\\mathcal X \\in \\mathbb R^{B\\times S\\times D}$ and $\\mathcal T \\in \\mathbb R^{B\\times D}$.\n",
    "This will automatically be achieved using the default PyTorch `DataLoader`.\n",
    "\n",
    "The datasets that we will use are provided here:\n",
    "\n",
    "GAIL stock: https://raw.githubusercontent.com/Pranavd0828/NIFTY50-StockMarket/main/Dataset/GAIL.csv\n",
    "\n",
    "NTPC stock: https://raw.githubusercontent.com/Pranavd0828/NIFTY50-StockMarket/main/Dataset/NTPC.csv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Targets Preprocessing\n",
    "\n",
    "\n",
    "Initially, we will load the datasets for GAIL and NTPC, selecting only the \"Date\" and \"Close\" features to be able to obtain $\\mathbf X$ and $\\mathbf T$. \n",
    "Then, we will normalize the value of prices since neural networks are sensitive to the scale of input features.\n",
    "Finally, we will reserve data from '2018-01-01' onwards from both datasets for testing purposes, where our objective is to predict future stock prices.\n",
    "\n",
    "For a given index $n$ into our data and a given sequence length $S$, we provide the input $\\mathbf X ^{[n]}$ and the target $\\mathbf T^{[n]}$ as follows:\n",
    "\n",
    "  $$\\mathbf X^{[n]} = \\{ \\vec x[t] \\ | \\ n \\leq t < n+S \\ | \\ 0 \\leq n < N-S \\}$$\n",
    "  $$\\mathbf T^{[n]} = \\vec x[n+S] $$\n",
    "\n",
    "where $\\mathbf N$ is the length of the data. \n",
    "\n",
    "\n",
    "For example, for the original data $[10,20,30,40,50,30,40,25,60,70,...]$, sequence length $S=5$ and index $n=2$, we would have the representations for $x = $ `[30,40,50,30,40]` and $t=$ `25`.\n",
    "\n",
    "Finally, we implement our own `Dataset` that returns the input and target."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data file\n",
    "\n",
    "Please run the code block below to download the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "# URLs for GAIL and NTPC stock CSV files\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/Pranavd0828/NIFTY50-StockMarket/main/Dataset/GAIL.csv\",\n",
    "    \"https://raw.githubusercontent.com/Pranavd0828/NIFTY50-StockMarket/main/Dataset/NTPC.csv\"\n",
    "]\n",
    "for url in urls:\n",
    "  filename = url.split(\"/\")[-1]\n",
    "\n",
    "  if not os.path.exists(filename):\n",
    "    urllib.request.urlretrieve(url)\n",
    "    print (\"Downloaded datafile: \", filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Data Loading\n",
    "\n",
    "Implement a function that:\n",
    "1. Loads/read all data from the .csv files `GAIL.csv` and `NTPC.csv` \n",
    "2. Choose only the `Date` and `Close` features to retrieve the date and price variables\n",
    "3. The `date` variable should be returned as a 1D NumPy array with the data type `numpy.datetime64()`\n",
    "4. The `price`  price should be returned as a 1D `Torch.Tensor`.\n",
    "\n",
    "Note:\n",
    "\n",
    "* Please make sure that you convert each date to specified dtype above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# load all data from the text file\n",
    "def get_data(datafile):\n",
    "\n",
    "    # Read/open datafile CSV file into a pandas DataFrame\n",
    "    data = pandas.read_csv(datafile)\n",
    "\n",
    "    # Extract date and convert to numpy array\n",
    "    date = ...\n",
    "\n",
    "    # Extract closing prices and convert to torch Tensor\n",
    "    price = ...\n",
    "\n",
    "    return date, price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read each datafile\n",
    "gail_data = get_data(...)\n",
    "ntpc_data = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Train Test Split\n",
    "\n",
    "Implement a function that splits the entire dataset into training and test sets:\n",
    "\n",
    "1. This function takes a tuple containing dates and corresponding prices as input.\n",
    "2. Prices corresponding to dates before `2018-01-01` should be allocated to the training set, while prices from `2018-01-01` onwards should be assigned to the test set.\n",
    "3. The function returns two tensors: one for the training prices and another for the test prices based on the split criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(stock_data):\n",
    "\n",
    "    ...\n",
    "    train_data = ...\n",
    "    test_data = ...\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# split the datas for both dataset\n",
    "gail_train,gail_test = train_test_split(...)\n",
    "ntpc_train,ntpc_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Data Normalization (Min Max Scaler) \n",
    "\n",
    "Implement two functions: one for scaling data, and the other for reversing this scaling.\n",
    "\n",
    "1. Scaling function will normalize the data with correct statistics `min` and `max`.\n",
    "2. Reverse scaling function will convert the scaled data back to its original scale \n",
    "3. With `min` and `max`, here is the formula for the scaling process:\n",
    "\n",
    "    $$X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}$$\n",
    "4. Normalize train and test sets of both datasets.\n",
    "\n",
    "Note:\n",
    "\n",
    "* When returning to the original values, ensure that you use the correct minimum and maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(train_data, test_data):\n",
    "\n",
    "    # Compute the correct statistics\n",
    "    min_val = ...\n",
    "    max_val = ...\n",
    "\n",
    "    # Scale the training data\n",
    "    train_data_scaled = ...\n",
    "\n",
    "    # Scale the test data using the same min and max values\n",
    "    test_data_scaled = ...\n",
    "\n",
    "    return train_data_scaled, test_data_scaled, min_val, max_val\n",
    "\n",
    "\n",
    "def inverse_min_max_scaler(scaled_data, min_val, max_val):\n",
    "\n",
    "    # Revert the scaling\n",
    "    original_data = ...\n",
    "\n",
    "    return original_data\n",
    "\n",
    "# scale the data for both datasets\n",
    "train_gail_scaled,test_gail_scaled,min_gail,max_gail = min_max_scaler(...)\n",
    "train_ntpc_scaled,test_ntpc_scaled,min_ntpc,max_ntpc = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Scaling\n",
    "This test ensures that both scaling and its inverse operation are implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_train_data = torch.ones(50)*20.\n",
    "random_train_data[:15] = -10.\n",
    "random_train_data[15:25] = 5.\n",
    "\n",
    "random_test_data = torch.ones(30)*25\n",
    "\n",
    "# scaling\n",
    "scaled_train_data,scaled_test_data,min_val,max_val = min_max_scaler(random_train_data,random_test_data)\n",
    "assert torch.sum(scaled_train_data) - 30 < 1e-5\n",
    "assert torch.sum(scaled_test_data) - 35 < 1e-5\n",
    "\n",
    "# reverse scaling\n",
    "reversed_train_data = inverse_min_max_scaler(scaled_train_data,min_val,max_val)\n",
    "reversed_test_data = inverse_min_max_scaler(scaled_test_data,min_val,max_val)\n",
    "assert all(reversed_train_data==random_train_data)\n",
    "assert all(reversed_test_data==random_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Plot the Stock Prices\n",
    "Plot the scaled stock prices of two datasets, distinguishing between train and test data points on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stock_data = {\n",
    "    'GAIL': (gail_data[0], train_gail_scaled, test_gail_scaled),  # gail_data[0] contains dates for GAIL\n",
    "    'NTPC': (ntpc_data[0], train_ntpc_scaled, test_ntpc_scaled)   # ntpc_data[0] contains dates for NTPC\n",
    "}\n",
    "\n",
    "# Create a figure with two subplots side by side (1 row, 2 columns)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "\n",
    "for idx, (..., (..., ..., ...)) in enumerate(stock_data.items()):\n",
    "    plt.subplot(1, 2, idx+1)\n",
    "\n",
    "    # Plot the training data on the left side\n",
    "    plt.plot(..., ..., label='Training')\n",
    "\n",
    "    # Plot the test data on the right side\n",
    "    plt.plot(..., ..., label='Test')\n",
    "\n",
    "    # Add a vertical line at 2018-01-01 for reference\n",
    "    plt.axvline(..., color='black', ls='--')\n",
    "\n",
    "    # Set labels and title for the subplot\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(...)\n",
    "\n",
    "    # Display legend for the plotted lines\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Sequences\n",
    "\n",
    "Convert the stock prices into sequences based on the specified sequence length $S$:\n",
    "\n",
    "1. Start the sequence from the initial time step 0.\n",
    "2. Each sequence includes the number of $S$ prices.\n",
    "3. The target for each sequence should be the price at the following time step (after $S$).\n",
    "4. The function should return two tensors: $X$ for the input sequences and $T$ for the target price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_targets(data: torch.Tensor, S):\n",
    "\n",
    "    # Initialize empty lists to hold the input sequences and the corresponding target values\n",
    "    X, T = [], []\n",
    "\n",
    "    # Go through the data to extract sequences based on S\n",
    "    ...\n",
    "\n",
    "    # Convert lists of sequences and targets into PyTorch tensors\n",
    "    return torch.stack(X), torch.stack(T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Check Sequences and Corresponding Targets\n",
    "\n",
    "Get all sequences and targets for a given data. This test assures that the X and T vectors are as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sequence and targets\n",
    "data = torch.arange(100,143)\n",
    "S = 5\n",
    "X,T = create_sequences_targets(data,S)\n",
    "\n",
    "assert (X.size(0)==len(data)-S) and X.size(1)==S\n",
    "assert X.size(0) == T.size(0)\n",
    "assert torch.sum(T)-4693 < 1e-5\n",
    "assert X[0,0]-100 < 1e-5 and X[-1,-1] - 141 < 1e-5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Dataset and Data Loader\n",
    "\n",
    "To enable batch processing, the sequence size must be the same for each element in the batch (otherwise it cannot be transformed as one large tensor). Thus, our dataset needs to have a fixed sequence size $S$. An exact value for $S$ can be selected by yourself.\n",
    "\n",
    "Implement a `Dataset` class derived from `torch.utils.data.Dataset` that provides $\\mathbf X^{[n]}$ and $\\mathbf T^{[n]}$. Implement three functions:\n",
    "\n",
    "1. The constructor `__init__(self, data, S)` that takes the `data` (stock prices) and (initial) sequence length $S$. It should create $X$ and $T$ here by utilizing the function in Task 5.\n",
    "2. The function `__len__(self)` that returns the number of samples in our dataset.\n",
    "3. Finally, the index function `__getitem__(self, index)` that returns the sequences $\\mathbf X^{[n]}$ and $\\mathbf T^{[n]}$ for a given `index`.\n",
    "\n",
    "After implementing the `Dataset`, initialize `DataLoader` for train and test sets of both datasets with batch size of $B=256$. For the test set, you can opt for the largest feasible batch size.\n",
    "\n",
    "Hint: When iterating through the dataloader, ensure that  $X$ is shaped as $\\mathbb R^{B\\times S\\times D}$.\n",
    "\n",
    "Note:\n",
    "* Be careful about `shuffle` parameter for test dataloaders and keep in mind that you should use scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, data, S):\n",
    "\n",
    "    # store the data and targets as required\n",
    "    ...\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # return input and target value for the given index\n",
    "    return ...\n",
    "\n",
    "  def __len__(self):\n",
    "    # return the length of this dataset\n",
    "    return ...\n",
    "\n",
    "# instantiate dataset and data loader for a reasonable sequence length S\n",
    "S = ...\n",
    "gail_train_dataset = Dataset(...)\n",
    "gail_train_dataloader = ...\n",
    "\n",
    "gail_test_dataset = ...\n",
    "gail_test_dataloader = ...\n",
    "\n",
    "ntpc_train_dataset = ...\n",
    "ntpc_train_dataloader = ...\n",
    "\n",
    "ntpc_test_dataset = ...\n",
    "ntpc_test_dataloader = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM Network Implementation\n",
    "\n",
    "In this section, we will explain the implementation of a simple LSTM (Long Short-Term Memory) network for sequence processing in PyTorch. This network is designed to process batches of sequences and predict outputs based on the sequence information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: LSTM Network Implementation\n",
    "\n",
    "Implement an LSTM network derived from `torch.nn.Module` class using `torch.nn.LSTM`, `torch.nn.Linear` and `torch.nn.Dropout` layers.\n",
    "\n",
    "1. In the constructor, instantiate all required layers for the given values of $D$ (input size), $K$ (hidden size), $O$ (output size).\n",
    "2. Network structure:\n",
    "    * One LSTM layer with input size $D$ and hidden size $K$. Please be careful about `batch_first` parameter when you consider the shape of the $X$ $(B\\times S\\times D)$.\n",
    "    * Dropout layer with the probability of $0.2$ on the LSTM network output.\n",
    "    * A Linear layer with the output neurons of $O$.\n",
    "3. Implement the input processing within the `forward` function of this LSTM network design. \n",
    "The `torch.nn.LSTM` layer returns three distinct elements. Refer to the documentation at https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html to understand the behavior of this layer.\n",
    "Ensure you comprehend how this layer in PyTorch operates before passing the appropriate input from the LSTM layer to the linear layer.\n",
    "\n",
    "Note:\n",
    "\n",
    "* $D$ and $O$ should be passed correctly while you can select any suitable number for $K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(torch.nn.Module):\n",
    "  def __init__(self, D, K, O):\n",
    "    super(LSTMModel,self).__init__()\n",
    "\n",
    "    self.lstm = ...\n",
    "    self.dropout = ...\n",
    "    self.linear = ...\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    # lstm layer\n",
    "\n",
    "    # apply dropout to the output of lstm layer\n",
    "    \n",
    "    # get correct element of the output of the lstm layer\n",
    "    Z = self.linear(...)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Training Loop\n",
    "\n",
    "Implement a function for the train loop. It takes network, dataloader, optimizer, loss function, device and the number epochs.\n",
    "\n",
    "If your dataset function from Task 5 is not fully compatible in terms of the shapes of logits and targets, you can use `squeeze()` or `unsqueeze()` operations to ensure smooth operation of the loss calculation. Alternatively, you may modify your dataset function to address this issue. Both works.\n",
    "\n",
    "Compute the average training loss per epoch and print it out.\n",
    "\n",
    "WARNING: Loss function will not complain when the index order for the output $\\mathcal Y$ and targets $\\mathcal T$ is incorrect, just the results will be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network,train_dataloader,optimizer,loss,device,epochs=50):\n",
    "    ...\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss = ...\n",
    "        total_sample = ...\n",
    "\n",
    "        for x, t in train_dataloader:\n",
    "            ...\n",
    "\n",
    "\n",
    "        # print average loss for training and validation\n",
    "        print(f\"\\rEpoch {epoch+1}; train loss: {train_loss/total_sample:1.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Run Training on Both Datasets\n",
    "\n",
    "* To train the LSTM network, you will decide the loss function when you consider the context of the problem.\n",
    "* Instantiate the optimizer with an appropriate learning rate $\\eta$.\n",
    "* Implement the training loop for $50$ epochs.\n",
    "* Run the network on the train sets of both datasets.\n",
    "\n",
    "Note that 50 epochs will not take long times, if implemented in an optimized way. So you may run on either GPU or CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ...\n",
    "loss = ...\n",
    "epochs = ...\n",
    "\n",
    "# for gail dataset\n",
    "lstm_gail_network = LSTMModel(...)\n",
    "optimizer_gail = ...\n",
    "train(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ntpc dataset, initialize the network again with the same D,K and O\n",
    "lstm_ntpc_network = LSTMModel(...)\n",
    "optimizer_ntpc = ...\n",
    "train(...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Future Price Prediction\n",
    "\n",
    "Write a function that predicts the next price of the sequence (test set) based on the values obtained from the network.\n",
    "\n",
    "This function executes the trained model on the test datasets from both datasets. It takes the trained model and the test data loader.\n",
    "\n",
    "Keep in mind that all predictions are also normalized, as the input data is normalized. These normalized predictions should be reversed to the original scale for visualization in the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network,test_dataloader):\n",
    "  predictions = ...\n",
    "\n",
    "  ...\n",
    "\n",
    "  return predictions\n",
    "\n",
    "# run the test for both datasets\n",
    "scaled_lstm_gail_preds = predict(...)\n",
    "scaled_lstm_ntpc_preds = predict(...)\n",
    "\n",
    "# reverse the test input and its predictions to original scales\n",
    "# reverse its predictions to original scales\n",
    "original_lstm_gail_preds = ...\n",
    "original_lstm_ntpc_preds = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11: Plotting Future Predictions\n",
    "\n",
    "Plot the actual prices and model predictions for only both test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "future_preds = {\n",
    "    'GAIL': (gail_test[S:], original_lstm_gail_preds),\n",
    "    'NTPC': (ntpc_test[S:], original_lstm_ntpc_preds)\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "for idx, (..., (..., ...)) in enumerate(future_preds.items()):\n",
    "    # Plot on the corresponding subplot (idx+1) within the 1x2 grid\n",
    "    plt.subplot(1, 2, idx+1)\n",
    "\n",
    "    # Plot the training data on the left side of the split subplot\n",
    "    plt.plot(..., label='Actual Price')\n",
    "\n",
    "    # Plot the test data on the right side of the split subplot\n",
    "    plt.plot(..., label='Model Prediction')\n",
    "\n",
    "    # Set labels and title for the subplot\n",
    "    plt.xlabel('Time (days)')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(...)\n",
    "\n",
    "    # Display legend for the plotted lines\n",
    "    plt.legend()\n",
    "\n",
    "    # Add gridlines with specified style, width, and color\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a29cabff5744fce69e08a959ab87b9e77a9f67b498d08783caa8c3bb16f23a00"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
