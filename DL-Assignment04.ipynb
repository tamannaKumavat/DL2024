{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Group Members:\n",
        "\n",
        "- Name, matriculation number\n",
        "- Name, matriculation number\n",
        "- Name, matriculation number"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9DKZj8jK_Dry"
      },
      "source": [
        "# Assignment 4: Multi-Output Networks and Batch Processing\n",
        "\n",
        "The goal of this exercise is to get to know some regularization techniques when implementing deep learning methods.\n",
        "For this purpose, we select a dataset that contains data in different formats, some binary ($x_d \\in \\{-1,1\\}$) and some numerical ($x_d\\in \\mathbb N$); and some are categorical, which we ignore for now.\n",
        "As target values, this dataset contains three numerical outputs, so, $\\vec t \\in \\mathbb R^3$ for each sample.\n",
        "These target values should be approximated with a two-layer multi-output network that we will train with the $\\mathcal J^{L_2}$ loss.\n",
        "\n",
        "Remember to make use of `numpy` in the matrix calculation, e.g. `numpy.dot`, `numpy.exp`, `numpy.mean` "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5lrQLreklXtS"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset of our choice is the Student Performance estimation dataset that was collected in Portugal in two different schools and with two different subjects, i.e., math and Portuguese (the mother tongue).\n",
        "The dataset contains many different inputs such as a binary representation of the school, gender, family sizes, and alike, as well as numerical representations of age, travel time, and alcohol consumption.\n",
        "The dataset also includes some categorical data, which we skip in this assignment.\n",
        "See https://archive.ics.uci.edu/ml/datasets/Student+Performance for more information on the dataset.\n",
        "As a start, we will rely on the Portuguese performance (`\"por\"`), but you can also try to use the Math samples (`\"mat\"`)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HSol7VoalR-7"
      },
      "source": [
        "### Task 1: Dataset Loading\n",
        "\n",
        "\n",
        "Load the dataset from files and provide the input matrix $\\mathbf X \\in \\mathbb R^{(D+1)\\times N}$ and the output matrix $\\mathbf T \\in \\mathbb R^{O\\times N}$.\n",
        "\n",
        "Due to the difficulty of the task, most of the implementation is provided.\n",
        "The implementation is very literal and, therefore, hopefully readable, while maybe not the most efficient.\n",
        "\n",
        "We skip categorical inputs (indexes 8-11) for now.\n",
        "All other entries are converted either into binary $(-1,1)$ or into an integer range $(0,1,\\ldots)$.\n",
        "The three outputs range between 0 and 20 each. The bias value for $x_0=1$ is also already included.\n",
        "You just need to make sure that the data $(X,T)$ is returned in the desired format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE3DlW-e_Dr1"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "# Dataset origin: https://archive.ics.uci.edu/ml/datasets/Student+Performance\n",
        "\n",
        "def dataset(course=\"por\"):\n",
        "  # load dataset and provide input and target data\n",
        "  # possible data files are \"mat\" and \"por\"\n",
        "\n",
        "  # download data file from URL\n",
        "  dataset_zip_file = \"student.zip\"\n",
        "  if not os.path.exists(dataset_zip_file):\n",
        "    import urllib.request\n",
        "    urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip\", dataset_zip_file)\n",
        "    print (\"Downloaded datafile\", dataset_zip_file)\n",
        "\n",
        "  import zipfile\n",
        "  import csv\n",
        "  import io\n",
        "\n",
        "  # collect inputs\n",
        "  inputs = []\n",
        "  targets = []\n",
        "  # some default values: yes=1, no=-1\n",
        "  yn = {\"yes\":1.,\"no\":-1.}\n",
        "  # read through dataset (without actually unzippiung to a file):\n",
        "  # ... open zip file\n",
        "  zip = zipfile.ZipFile(dataset_zip_file)\n",
        "  # ... open data file inside of zip file and convert bytes to text\n",
        "  datafile = io.TextIOWrapper(zip.open(os.path.join(F\"student-{course}.csv\"), 'r'))\n",
        "  # ... read through the lines via CSV reader, using the correct delimiter\n",
        "  reader = csv.reader(datafile, delimiter=\";\")\n",
        "  # ... skip header line\n",
        "  next(reader)\n",
        "  for splits in reader:\n",
        "    # read input values\n",
        "    inputs.append([\n",
        "      1.,                             #### BIAS ####\n",
        "      {\"GP\":1.,\"MS\":-1.}[splits[0]],  # school\n",
        "      {\"M\":1.,\"F\":-1.}[splits[1]],    # gender\n",
        "      float(splits[2]),               # age\n",
        "      {\"U\":1.,\"R\":-1.}[splits[3]],    # address\n",
        "      {\"LE3\":1.,\"GT3\":-1.}[splits[4]],# family size\n",
        "      {\"T\":1.,\"A\":-1.}[splits[5]],    # parents living together\n",
        "      float(splits[6]),               # mother education\n",
        "      float(splits[7]),               # father education\n",
        "      # skip categorical values\n",
        "      float(splits[12]),              # travel time\n",
        "      float(splits[13]),              # study time\n",
        "      float(splits[14]),              # failures\n",
        "      yn[splits[15]],                 # extra support\n",
        "      yn[splits[16]],                 # family support\n",
        "      yn[splits[17]],                 # paid support\n",
        "      yn[splits[18]],                 # activities\n",
        "      yn[splits[19]],                 # nursery school\n",
        "      yn[splits[20]],                 # higher education\n",
        "      yn[splits[21]],                 # internet\n",
        "      yn[splits[22]],                 # romantic\n",
        "      float(splits[23]),              # family relation\n",
        "      float(splits[24]),              # free time\n",
        "      float(splits[25]),              # going out\n",
        "      float(splits[26]),              # workday alcohol\n",
        "      float(splits[27]),              # weekend alcohol\n",
        "      float(splits[28]),              # health\n",
        "      float(splits[29]),              # absences\n",
        "    ])\n",
        "\n",
        "    # read targets values\n",
        "    targets.append([\n",
        "      float(splits[30]),              # grade for primary school\n",
        "      float(splits[31]),              # grade for secondary school\n",
        "      float(splits[32]),              # grade for tertiary school\n",
        "    ])\n",
        "\n",
        "  print(F\"Loaded dataset with {len(targets)} samples\")\n",
        "  return ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWNwt2Rb_Dr3"
      },
      "source": [
        "### Test 1: Assert Valid Outputs\n",
        "\n",
        "This test will check the dimension of the loaded dataset, i.e. $\\mathbf X\\in \\mathbb R^{(D+1)\\times N}$ and $\\mathbf T \\in \\mathbb R^{O\\times N}$, and also assure that all target data is in the range $t\\in[0,20]$.\n",
        "\n",
        "Please make sure that your implementation can pass these tests before moving to the next task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaCpzc-l_Dr3",
        "outputId": "6fefb52a-492d-489a-aec3-156623dd939c"
      },
      "outputs": [],
      "source": [
        "X_orig, T = dataset(\"por\")\n",
        "\n",
        "assert numpy.all(T >= 0) and numpy.all(T <= 20)\n",
        "\n",
        "assert X_orig.shape[0] == 27\n",
        "assert T.shape[0] == 3\n",
        "assert T.shape[1] == X_orig.shape[1]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dfk_P5eG_Dr3"
      },
      "source": [
        "### Task 2: Input Data Normalization (Min-Max)\n",
        "\n",
        "Since the data is in different input regimes, we want to normalize the data.\n",
        "For this purpose, we need to compute the minimum and the maximum of the data for each input dimension.\n",
        "Then, we implement a function to perform the normalization of the data using the previously computed minimum and maximum. Make sure that you handle the bias neuron $x_0$ correctly. The formula for min-max normalization is provided below:\n",
        "\n",
        "  $$\\forall d >= 1\\colon x_d^{\\text{norm}} = \\frac{x_d - \\min\\limits_{\\vec x'\\in \\mathbf X^T} x_d'}{\\max\\limits_{\\vec x'\\in \\mathbf X^T} x_d' - \\min\\limits_{\\vec x'\\in \\mathbf X^T} x_d'}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bUpUAHR_Dr3"
      },
      "outputs": [],
      "source": [
        "# compute maximum and minimum over dataset\n",
        "min_val = ...\n",
        "max_val = ...\n",
        "\n",
        "# assure to handle x_0 correctly\n",
        "...\n",
        "\n",
        "def normalize(x, min_val, max_val):\n",
        "  # normalize the given data with the given minimum and maximum values\n",
        "  return ...\n",
        "\n",
        "# Normalize our dataset\n",
        "X = normalize(X_orig, min_val, max_val)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6MZ_MPc-_Dr4"
      },
      "source": [
        "### Task 3: Batch Processing\n",
        "\n",
        "In order to run stochastic gradient descent, we need to split our dataset into batches of a certain batch size $B$. Implement a function that turns the dataset $(\\mathbf X, \\mathbf T)$ into batches of a certain batch size $B$.\n",
        "Implement this function as a generator function, i.e., use ``yield`` instead of ``return``.\n",
        "Circulate the dataset afresh when all data is consumed, and shuffle the data in each epoch.\n",
        "Make sure that you yield both the input batch and the target batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoDHo80J_Dr4"
      },
      "outputs": [],
      "source": [
        "def batch(X, T, batch_size=16):\n",
        "  ...\n",
        "  while ...:\n",
        "    # shuffle dataset in each epoch\n",
        "    ...\n",
        "    # yield the batch\n",
        "    yield ..., ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NgBsBOsa_Dr4"
      },
      "source": [
        "### Test 2: Test your Batches\n",
        "\n",
        "This test is to assure that your batch generation function works as expected. \n",
        "We define some test data for this purpose.\n",
        "The code below checks whether your batch function returns batches with correct content, i.e., $(\\vec x, \\vec t)$-alignment. \n",
        "It also checks that the batches are in the correct dimensions, i.e., that $\\mathbf X \\in \\mathbb R^{(D+1)\\times B}$ and $\\mathbf T \\in \\mathbb R^{O\\times B}$.\n",
        "\n",
        "Make sure you can pass this test before moving forward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFndxbf1_CmR"
      },
      "outputs": [],
      "source": [
        "XX = numpy.array([[i] * 5 for i in range(50)]).T\n",
        "TT = numpy.array([[i] for i in range(10,60)]).T\n",
        "\n",
        "for counter, (x,t,e) in enumerate(batch(XX, TT, 16)):\n",
        "  assert x.shape[0] == 5\n",
        "  assert x.shape[1] == 16\n",
        "  assert t.shape[0] == 1\n",
        "  assert t.shape[1] == 16\n",
        "  assert numpy.all(x == t-10)\n",
        "  assert e == (counter % 3 == 0)\n",
        "  if counter == 20: break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0B8o_mE1l6mp"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "To train a two-layer multi-output regression network, we need to implement some functions.\n",
        "The network output is computed in three steps:\n",
        "\n",
        "  * Compute network activation for a batch of inputs $\\mathbf X$: $\\mathbf A = \\mathbf W^{(1)}\\mathbf X$\n",
        "  * Call the activation function element-wise: $\\mathbf H = g(\\mathbf A)$. Here, we rely on the logistic activation function $\\sigma$. Assure that the hidden neuron bias $\\mathbf H_{0,:}$ is set appropriately.\n",
        "  * Compute the output $\\mathbf Y$ of the batch: $\\mathbf Y = \\mathbf W^{(2)}\\mathbf H$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0pGCwC0L_Dr4"
      },
      "source": [
        "### Task 4: Multi-Output Network\n",
        "\n",
        "Implement a multi-target network that computes the output matrix $\\mathbf Y$ for a given input dataset/batch $\\mathbf X$ and given parameters $\\Theta=(\\mathbf W^{(1)}, \\mathbf W^{(2)})$ using `numpy` operations. \n",
        "The function should return both the output $\\mathbf Y$ and the output of the hidden units $\\mathbf H$ since we will need these in gradient descent. Select the logistic function $\\sigma$ as the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1gErN4z_Dr4"
      },
      "outputs": [],
      "source": [
        "def network(X, Theta):\n",
        "  W1, W2 = Theta\n",
        "\n",
        "  # compute activation\n",
        "  A = ...\n",
        "\n",
        "  # compute hidden unit output\n",
        "  H = ...\n",
        "\n",
        "  # compute network output\n",
        "  Y = ...\n",
        "\n",
        "  return Y, H"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt0CMab4_Dr5"
      },
      "source": [
        "### Task 5: Loss Implementation\n",
        "\n",
        "Implement a loss function that returns the squared loss $\\mathcal J^{L_2} = \\frac1B \\|\\mathbf Y - \\mathbf T\\|_F^2$ for given network outputs $\\mathbf Y$ and target values $\\mathbf T$.\n",
        "Use `numpy` or `scipy` functionality for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSwKiqIc_Dr5"
      },
      "outputs": [],
      "source": [
        "def loss(Y, T):\n",
        "  return ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fD2srCKN_Dr5"
      },
      "source": [
        "### Task 6: Gradient Implementation\n",
        "\n",
        "Implement a function that computes and returns the gradient for a given batch $(\\mathbf X, \\mathbf T)$, the given network outputs $\\mathbf Y$ and $\\mathbf H$ as well as current parameters $\\Theta=(\\mathbf W^{(1)}, \\mathbf W^{(2)})$.\n",
        "Make sure to compute the gradient with respect to both weight matrices. Remember that we have used $\\sigma$ as the activation function.\n",
        "Implement the function using the fast version provided in the lecture and make use of `numpy` operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe8QYVnP_Dr5"
      },
      "outputs": [],
      "source": [
        "def gradient(X, T, Y, H, Theta):\n",
        "  W1, W2 = Theta\n",
        "\n",
        "  # first layer gradient\n",
        "  g1 = ...\n",
        "  # second layer gradient\n",
        "  g2 = ...\n",
        "\n",
        "  return g1, g2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CpeCOHbE_Dr5"
      },
      "source": [
        "### Task 7: Iterative Gradient Descent\n",
        "\n",
        "\n",
        "Implement gradient descent for a given number of 10'000 epochs (**not batches!**) using given initial parameters $\\Theta$ and a given batch size $B$, as well as a learning rate of $\\eta=0.001$.\n",
        "\n",
        "Make use of the normalized dataset from Task 2, split into batches with the function from Task 3, the network from Task 4, the loss from Task 5, and the gradient from Task 6.\n",
        "\n",
        "Make sure that the network output $\\mathbf Y$ and the hidden unit output $\\mathbf H$ are computed only once for each batch. After applying gradient descent, add an option to use momentum learning with the given parameter `mu`.\n",
        "At the end of each epoch, compute and store the loss values for each batch in a list, and this list will be returned at the end.\n",
        "\n",
        "How many iterations do we need when $B < N$? How can you know whether your current batch is the last one of the current epoch?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnuFqkg-82l2"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, T, Theta, B, eta=0.001, mu=None):\n",
        "  loss_values = []\n",
        "\n",
        "  max_epochs = 10000\n",
        "  max_batches = ...\n",
        "\n",
        "  # iterate over batches\n",
        "  for x,t in ...:\n",
        "    # compute network output\n",
        "    ...\n",
        "    # compute and append loss\n",
        "    ...\n",
        "\n",
        "    # compute gradient\n",
        "    ...\n",
        "    # and apply gradient descent\n",
        "    ...\n",
        "\n",
        "    # apply momentum learning if desired\n",
        "    ...\n",
        "\n",
        "  # return the obtained loss values at the end\n",
        "  return loss_values"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NIvoljpW_Dr5"
      },
      "source": [
        "### Task 8: Run Gradient Descent\n",
        "\n",
        "Select an appropriate number of hidden neurons $K$.\n",
        "Instantiate the weight matrices $\\Theta=(\\mathbf W^{(1)}, \\mathbf W^{(2)})$ using the Xavier method as introduced in the lecture.\n",
        "\n",
        "Run the gradient descent three times, first as normal gradient descent, second as stochastic gradient descent with batch size $B=16$, and third with the same setup as the second but with momentum learning involved, select $\\mu =0.9$.\n",
        "\n",
        "How can you achieve this without requiring separate implementations of the ``gradient_descent`` function from Task 7?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4BnX1m8_Dr6"
      },
      "outputs": [],
      "source": [
        "K = ...\n",
        "D = ...\n",
        "O = ...\n",
        "W1 = ...\n",
        "W2 = ...\n",
        "Theta = [W1, W2]\n",
        "\n",
        "import copy\n",
        "\n",
        "# run gradient descent with full dataset\n",
        "Theta1 = copy.deepcopy(Theta)\n",
        "GD = ...\n",
        "\n",
        "# run stochastic gradient descent with batches of size 16\n",
        "Theta2 = copy.deepcopy(Theta)\n",
        "SGD = ...\n",
        "\n",
        "# run stochastic gradient descent with batches of size 16 and momentum mu=0.9\n",
        "Theta3 = copy.deepcopy(Theta)\n",
        "SGD_Mo = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi5Wt88CnsNB"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Finally, we want to evaluate how the learning process went and what the network has actually learned.\n",
        "For the former, we will plot the loss values obtained during training.\n",
        "For the latter, we define one specific sample of our own, and we evaluate the impact of several factors on the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1CGUw7k_Dr6"
      },
      "source": [
        "### Task 9: Plotting Loss Progression\n",
        "\n",
        "To show the learning process of the networks, plot the loss values of the three gradient descent steps from Task 8 together into one plot.\n",
        "Do we need to take care of something when plotting both together?\n",
        "\n",
        "Use logarithmic axes wherever you see fit.\n",
        "An exemplary loss progression plot can be found in the slides.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMFMvuRg_Dr6"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "pyplot.plot(..., \"b-\", label=\"Stochastic Gradient Descent\")\n",
        "pyplot.plot(..., \"r-\", label=\"Stochastic Gradient Descent with Momentum\")\n",
        "pyplot.plot(..., \"g-\", label=\"Gradient Descent\")\n",
        "pyplot.legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sDJp99mK_Dr6"
      },
      "source": [
        "### Task 10: Best and Worst Performers\n",
        "\n",
        "We want to see what the network has learned.\n",
        "Therefore, we evaluate the best student in our dataset as well as the worst student based on their average grade. \n",
        "The best student would be the one with the highest average of the 3 outputs from the model and the worst student would be the one with the lowest average of the 3 outputs from the model.\n",
        "We will look into the features that represent the best student and the worst. \n",
        "\n",
        "Remember that input data need to be normalized before feeding it to the network. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variables to store your info on best and worst student\n",
        "best_student_idx = ... # Index of the top student from the dataset\n",
        "worst_student_idx = ... # Index of the bottom student from the dataset\n",
        "current_best_score = ... # Best average score of the top student\n",
        "current_worst_score = ... # Worst average score of the bottom student\n",
        "\n",
        "# Iterate over the whole dataset\n",
        "for i, x in enumerate(X.T):\n",
        "\n",
        "    # compute network predictions\n",
        "    prediction = ...\n",
        "\n",
        "    # Take the average of the predictions\n",
        "    avg = ...\n",
        "\n",
        "    # Check for top student. (Save their index and their average scores)\n",
        "    ...\n",
        "\n",
        "    # Check for bottom student. (Save their index and their average scores)\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Use the code below to see the features of top and bottom student in a table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best and worst student featuers\n",
        "best_student = X_orig[:, best_student_idx]\n",
        "worst_student = X_orig[:, worst_student_idx]\n",
        "\n",
        "# Append the average prediction in the above arrays for both students\n",
        "best_student_with_score = numpy.append(best_student, current_best_score)\n",
        "worst_student_with_score = numpy.append(worst_student, current_worst_score)\n",
        "\n",
        "# This is for your visualization purpose only. It will show you the top and bottom student in a tabular format.\n",
        "import pandas as pd\n",
        "\n",
        "# Set the option to display all columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "#Just for visualization purposes\n",
        "columns = [\"BIAS\", \"School\", \"Gender\", \"Age\", \"Address\", \"Small family\", \"Parents Living Together\", \"Mother with higher education\", \"Father with secondary education\",\n",
        "           \"Travel time\", \"Study time\", \"Failures\", \"School Support\", \"Family Support\", \"Additional Classes\", \"Out-of-school activities\",\n",
        "           \"Nursery school\", \"Higher education\", \"Internet access\", \"Romantic Relationship\", \"Good relation with family\", \"Free time\",\n",
        "           \"Goes out with peers\", \"Workday alcohol consumption\", \"Weekend alcohol consumption\", \"Current health status\",\n",
        "           \"Absences\", \"Average Score\"]\n",
        "\n",
        "# Initialize an empty DataFrame with the specified columns\n",
        "df = pd.DataFrame(numpy.vstack([best_student_with_score, worst_student_with_score]), columns=columns)\n",
        "df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9lB1b-8V_Dr6"
      },
      "source": [
        "### Task 11: Influence of Data Dimensions (Optional)\n",
        "\n",
        "\n",
        "For some dimensions in the input feature $\\vec x$, we want to test how different input values for this dimension would influence the outcome.\n",
        "Particularly, we test:\n",
        "\n",
        "  * Weekly study time at index $d=10$: vary in the range $[1,4]$ \n",
        "  * Past Failures at index $d=11$: vary in range $[0,3]$ \n",
        "  * Romantic relations at index $d=19$: change between yes ($1$) and no ($-1$)\n",
        "  * Weekday alcohol consumption at index $d=23$: varies in the range $[1,6]$.\n",
        "  * Absences at index $d=26$: vary with following values $[0,10,20,30]$.\n",
        "\n",
        "Note that the indexes include the fact that we are omitting some input dimensions, so they might differ from what is listed on the webpage.\n",
        "\n",
        "Check how the performance changes if the above features are changed for the best student and the worst student. \n",
        "\n",
        "Did the average increase or decrease? Did you expect this output?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buTufJpd_Dr6"
      },
      "outputs": [],
      "source": [
        "# implement a way to modify the input at a given index with certain values\n",
        "# and to predict and print the network output for this modification\n",
        "...\n",
        "# run this with the 5 modifications and their according to values as seen above for both top and bottom student\n",
        "..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('DL')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
